{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *\n",
    "import os\n",
    "\n",
    "if torch.cuda.is_available() == False:\n",
    "    raise ValueError(\"No CUDA device found!\")\n",
    "    \n",
    "plt.style.use('dark_background')\n",
    "\n",
    "path = untar_data(\"http://vision.ucsd.edu/datasets/yale_face_dataset_original/yalefaces.zip\")\n",
    "\n",
    "# redundant file\n",
    "if os.path.exists(path/\"subject01.glasses.gif\"):\n",
    "    os.remove(path/\"subject01.glasses.gif\") \n",
    "    \n",
    "# incorrect naming    \n",
    "if os.path.exists(path/\"subject01.gif\"):\n",
    "    os.rename(path/\"subject01.gif\", path/\"subject01.centerlight\")\n",
    "\n",
    "files = L(path.glob(\"subject*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseImage(fastuple):\n",
    "    def show(self, ctx=None, **kwargs):\n",
    "        img1, img2, same = self\n",
    "        if not isinstance(img1, Tensor):\n",
    "            if img2.size != img1.size: img2 = img2.resize(img1.size)\n",
    "            t1, t2 = tensor(img1), tensor(img2)\n",
    "            t1, t2 = t1.permute(2,0,1), t2.permute(2,0,1)\n",
    "        else: t1, t2 = img1, img2\n",
    "        line = t1.new_zeros(t1.shape[0], t1.shape[1], 10)\n",
    "        return show_image(torch.cat([t1, line, t2], dim=2), title=same, ctx=ctx)\n",
    "    \n",
    "class SiameseTransform(Transform):\n",
    "    def __init__(self, files, label_func, splits):\n",
    "        self.labels = files.map(label_func).unique()        \n",
    "        self.lbl2files = {l : L(f for f in files if label_func(f) == l) for l in self.labels}\n",
    "        self.label_func = label_func\n",
    "        self.valid = {f: self._draw(f) for f in files[splits[1]]}\n",
    "        \n",
    "    def encodes(self, f):\n",
    "        f2, t = self.valid.get(f, self._draw(f)) # calls draw() if key=f does not exist\n",
    "        img1, img2 = PILImage.create(f), PILImage.create(f2)\n",
    "        return SiameseImage(img1, img2, t)\n",
    "    \n",
    "    def _draw(self, f):\n",
    "        same = random.random() < 0.5\n",
    "        cls = self.label_func(f)\n",
    "        if not same:\n",
    "            cls = random.choice(L(l for l in self.labels if l != cls))\n",
    "        return random.choice(self.lbl2files[cls]), same\n",
    "              \n",
    "class SiameseModel(Module):\n",
    "    def __init__(self, encoder):\n",
    "        self.encoder = encoder\n",
    "        self.fc = nn.Linear(1024, 1)\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        e1 = self.encoder(x1)\n",
    "        e2 = self.encoder(x2)\n",
    "        \n",
    "        x = torch.abs(e1 - e2)\n",
    "        x = self.fc(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        \n",
    "        return x        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(out, target):\n",
    "    return nn.BCELoss()(torch.squeeze(out, 1), target.float())\n",
    "\n",
    "def my_accuracy(input, target):\n",
    "    label = input > 0.5\n",
    "    return (label.squeeze(1) == target).float().mean()\n",
    "\n",
    "def label_func(fname):\n",
    "    return re.match(r'^subject(.*)\\.', fname.name).groups()[0]\n",
    "\n",
    "# make a test set\n",
    "# our test will have faces with glasses, these won't be in the training\n",
    "test_files = L()\n",
    "train_files = L()\n",
    "\n",
    "for f in files:\n",
    "    if \".glasses\" in f.name:\n",
    "        test_files.append(f)\n",
    "    else:\n",
    "        train_files.append(f)\n",
    "            \n",
    "splits = RandomSplitter()(train_files)\n",
    "tfm = SiameseTransform(train_files, label_func, splits)\n",
    "tls = TfmdLists(train_files, tfm, splits=splits)\n",
    "dls = tls.dataloaders(after_item=[ToTensor], after_batch=[IntToFloatTensor, Normalize.from_stats(*imagenet_stats)], bs=16)\n",
    "\n",
    "encoder = nn.Sequential(\n",
    "    create_body(resnet18, cut=-2),\n",
    "    AdaptiveConcatPool2d(),\n",
    "    nn.Flatten()\n",
    ")\n",
    "\n",
    "model = SiameseModel(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(dls, model, loss_func=my_loss, metrics=my_accuracy).to_fp16()\n",
    "#learn.lr_find()\n",
    "learn.fit(50, 1e-3)\n",
    "learn.save(\"yale_face\")\n",
    "learn.recorder.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fastai.learner.Learner at 0x7f1ae94ebf90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn = Learner(dls, model, loss_func=my_loss, metrics=my_accuracy).to_fp16()\n",
    "learn.model.cuda()\n",
    "learn.load(\"yale_face\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject01.glasses most similar to label 01\n",
      "subject02.glasses most similar to label 02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-c36f2229c18c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIntToFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimagenet_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/fastai/torch_core.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;31m#         if func.__name__[0]!='_': print(func, types, args, kwargs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;31m#         with torch._C.DisableTorchFunction(): ret = _convert(func(*args, **(kwargs or {})), self.__class__)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__torch_function__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorBase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_copy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDisableTorchFunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# For every test image we compare against all the training images.\n",
    "# Each training image will vote based using the similarity score from the network.\n",
    "# This is sort of like K nearest neighbour, where K is the number of training images.\n",
    "# I'm doing this in a very slow nested loop to keep it simple!\n",
    "correct = 0\n",
    "\n",
    "for f1 in sorted(test_files):\n",
    "    img1 = PILImage.create(f1)\n",
    "\n",
    "    # disable gradients to save GPU memory!\n",
    "    with torch.no_grad(): \n",
    "        x1 = ToTensor()(img1).cuda()\n",
    "        x1 = IntToFloatTensor()(x1)\n",
    "        x1 = Normalize.from_stats(*imagenet_stats)(x1)\n",
    "\n",
    "    vote = {}\n",
    "    \n",
    "    for f2 in train_files:\n",
    "        img2 = PILImage.create(f2)\n",
    "        label = label_func(f2)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x2 = ToTensor()(img2).cuda()\n",
    "            x2 = IntToFloatTensor()(x2)\n",
    "            x2 = Normalize.from_stats(*imagenet_stats)(x2)\n",
    "            \n",
    "            out = learn.model(x1, x2)\n",
    "            vote[label] = vote.get(label, 0.0) + out\n",
    "        \n",
    "    best_label = max(vote, key=vote.get)\n",
    "    \n",
    "    if label_func(f1) == best_label:\n",
    "        correct += 1\n",
    "        \n",
    "    print(f\"{f1.name} most similar to label {best_label}\")\n",
    "    \n",
    "print(f\"correct classification {correct}/{len(test_files)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
